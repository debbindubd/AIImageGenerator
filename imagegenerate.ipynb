{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 10:56:12.740878: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-12 10:56:12.747813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-12 10:56:12.756199: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-12 10:56:12.758678: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-12 10:56:12.765012: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-12 10:56:14.383 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.384 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.412 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/bindu/anaconda3/envs/ptenv/lib/python3.12/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-11-12 10:56:14.413 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.413 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.414 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.414 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.414 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.415 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.415 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.416 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.416 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.416 Session state does not function when running a script without `streamlit run`\n",
      "2024-11-12 10:56:14.417 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.417 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.417 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.417 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.418 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.418 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.419 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.419 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.419 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.420 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.420 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.420 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-12 10:56:14.420 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Set page config\n",
    "st.set_page_config(page_title=\"AI Image Generator\", layout=\"wide\")\n",
    "\n",
    "# Check GPU availability\n",
    "def check_gpu():\n",
    "    if not torch.cuda.is_available():\n",
    "        st.error(\"CUDA is not available. Using CPU instead.\")\n",
    "        return False\n",
    "    st.success(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    return True\n",
    "\n",
    "# Function to generate image\n",
    "@st.cache_resource\n",
    "def load_model(model_id):\n",
    "    try:\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16  # Using float16 for GPU memory efficiency\n",
    "        )\n",
    "        if check_gpu():\n",
    "            pipe = pipe.to(\"cuda\")\n",
    "            # Enable memory efficient attention\n",
    "            pipe.enable_attention_slicing()\n",
    "            pipe.enable_xformers_memory_efficient_attention()\n",
    "        return pipe\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def generate_image(pipe, prompt):\n",
    "    with torch.cuda.amp.autocast():  # Automatic mixed precision\n",
    "        image = pipe(prompt).images[0]\n",
    "    return image\n",
    "\n",
    "def main():\n",
    "    st.title(\"AI Image Generator\")\n",
    "    \n",
    "    # Display GPU info\n",
    "    if check_gpu():\n",
    "        st.info(f\"GPU Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Input components\n",
    "    prompt = st.text_input(\"Enter your prompt:\")\n",
    "    model_option = st.selectbox(\n",
    "        \"Select Model\",\n",
    "        [\"runwayml/stable-diffusion-v1-5\", \"stabilityai/stable-diffusion-2-1\"]\n",
    "    )\n",
    "    \n",
    "    # Generate button\n",
    "    if st.button(\"Generate Image\"):\n",
    "        if prompt:\n",
    "            with st.spinner(\"Generating image...\"):\n",
    "                try:\n",
    "                    # Load model\n",
    "                    pipe = load_model(model_option)\n",
    "                    if pipe is None:\n",
    "                        return\n",
    "                    \n",
    "                    # Generate image\n",
    "                    image = generate_image(pipe, prompt)\n",
    "                    \n",
    "                    # Display image\n",
    "                    st.image(image, caption=\"Generated Image\", use_column_width=True)\n",
    "                    \n",
    "                    # Clear GPU memory\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    st.error(f\"Error generating image: {str(e)}\")\n",
    "        else:\n",
    "            st.warning(\"Please enter a prompt first!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from datetime import datetime\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Create directories\n",
    "CACHE_DIR = \"model_cache\"\n",
    "OUTPUT_DIR = \"generated_images\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def check_gpu():\n",
    "    \"\"\"Check GPU availability and return status\"\"\"\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error checking GPU: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def save_image(image, prompt):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    safe_prompt = \"\".join(x for x in prompt[:30] if x.isalnum() or x in (' ','-','_')).strip()\n",
    "    filename = f\"{timestamp}_{safe_prompt}.png\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    image.save(filepath)\n",
    "    return filepath\n",
    "\n",
    "# Update this list with the RobMix Zenith identifier\n",
    "model_option = st.selectbox(\n",
    "    \"Select Model\",\n",
    "    [\"stabilityai/stable-diffusion-2-1\", \"civitai/robmix-zenith\"]\n",
    ")\n",
    "\n",
    "def download_model(model_id):\n",
    "    \"\"\"Download model once and cache it\"\"\"\n",
    "    local_path = os.path.join(CACHE_DIR, model_id.split('/')[-1])\n",
    "    \n",
    "    # Ensure compatibility with both Hugging Face and CivitAI identifiers\n",
    "    if model_id == \"civitai/robmix-zenith\":\n",
    "        # Assuming you've downloaded RobMix Zenith manually or via a CivitAI downloader\n",
    "        st.info(f\"Using locally available RobMix Zenith model: {local_path}\")\n",
    "    elif not os.path.exists(local_path):\n",
    "        st.info(f\"Downloading model {model_id} (this will happen only once)...\")\n",
    "        snapshot_download(\n",
    "            repo_id=model_id,\n",
    "            local_dir=local_path,\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "    return local_path\n",
    "\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model(model_id):\n",
    "    try:\n",
    "        local_path = download_model(model_id)\n",
    "        \n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            local_path,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            local_files_only=True,\n",
    "            safety_checker=None,  # Disable safety checker\n",
    "            requires_safety_checker=False\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            pipe = pipe.to(\"cuda\")\n",
    "            pipe.enable_attention_slicing()\n",
    "            pipe.enable_xformers_memory_efficient_attention()  # Enable memory efficient attention\n",
    "        return pipe\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def generate_image(pipe, prompt):\n",
    "    try:\n",
    "        # Enhance the prompt\n",
    "        enhanced_prompt = f\"high quality, detailed, 4k, {prompt}\"\n",
    "        negative_prompt = \"ugly, blurry, bad quality, dark, low resolution, deformed, distorted\"\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            with torch.autocast(\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "                image = pipe(\n",
    "                    prompt=enhanced_prompt,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                    num_inference_steps=50,  # Increased steps\n",
    "                    guidance_scale=8.5,      # Increased guidance\n",
    "                    width=512,\n",
    "                    height=512,\n",
    "                ).images[0]\n",
    "                \n",
    "                # Verify image generation\n",
    "                if image is None or image.getextrema() == (0, 0) or image.getextrema() == (255, 255):\n",
    "                    raise Exception(\"Generated image is invalid\")\n",
    "                \n",
    "                return image\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error during image generation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    st.title(\"AI Image Generator\")\n",
    "    \n",
    "    # Add memory management at start\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    if check_gpu():\n",
    "        st.success(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        st.info(f\"GPU Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        st.warning(\"No GPU detected. Using CPU (this will be slower)\")\n",
    "    \n",
    "    prompt = st.text_area(\"Enter your prompt (be descriptive):\", \n",
    "                         help=\"Example: A beautiful mountain landscape at sunset, with realistic details, 4k quality\")\n",
    "    \n",
    "    model_option = st.selectbox(\n",
    "        \"Select Model\",\n",
    "        [\"stabilityai/stable-diffusion-2-1\"]\n",
    "    )\n",
    "    \n",
    "    if st.button(\"Generate Image\"):\n",
    "        if not prompt or len(prompt.strip()) < 3:\n",
    "            st.warning(\"Please enter a detailed prompt (at least 3 characters)!\")\n",
    "            return\n",
    "            \n",
    "        with st.spinner(\"Generating image... This may take a minute...\"):\n",
    "            try:\n",
    "                # Clear GPU memory before generation\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                pipe = load_model(model_option)\n",
    "                if pipe is None:\n",
    "                    return\n",
    "                \n",
    "                image = generate_image(pipe, prompt)\n",
    "                if image is None:\n",
    "                    return\n",
    "                \n",
    "                filepath = save_image(image, prompt)\n",
    "                st.image(image, caption=f\"Generated Image for: {prompt}\", use_container_width=True)\n",
    "                \n",
    "                with open(filepath, \"rb\") as file:\n",
    "                    btn = st.download_button(\n",
    "                        label=\"Download Image\",\n",
    "                        data=file,\n",
    "                        file_name=os.path.basename(filepath),\n",
    "                        mime=\"image/png\"\n",
    "                    )\n",
    "                \n",
    "                st.success(f\"Image saved to: {filepath}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                st.error(f\"Error: {str(e)}\")\n",
    "            finally:\n",
    "                # Clean up memory\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "def download_all_models():\n",
    "    models = [\n",
    "        # \"runwayml/stable-diffusion-v1-5\",\n",
    "        \"stabilityai/stable-diffusion-2-1\"\n",
    "    ]\n",
    "    \n",
    "    for model_id in models:\n",
    "        download_model(model_id)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if st.button(\"Download All Models\"):\n",
    "        download_all_models()\n",
    "        st.success(\"All models downloaded successfully!\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all five model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from datetime import datetime\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Create directories\n",
    "CACHE_DIR = \"model_cache\"\n",
    "OUTPUT_DIR = \"generated_images\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def check_gpu():\n",
    "    \"\"\"Check GPU availability and return status\"\"\"\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error checking GPU: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def save_image(image, prompt):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    safe_prompt = \"\".join(x for x in prompt[:30] if x.isalnum() or x in (' ','-','_')).strip()\n",
    "    filename = f\"{timestamp}_{safe_prompt}.png\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    image.save(filepath)\n",
    "    return filepath\n",
    "\n",
    "def download_model(model_id):\n",
    "    \"\"\"Download model once and cache it\"\"\"\n",
    "    local_path = os.path.join(CACHE_DIR, model_id.split('/')[-1])\n",
    "    \n",
    "    if not os.path.exists(local_path):\n",
    "        st.info(f\"Downloading model {model_id} (this will happen only once)...\")\n",
    "        snapshot_download(\n",
    "            repo_id=model_id,\n",
    "            local_dir=local_path,\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "    return local_path\n",
    "\n",
    "# Add these models to the available options\n",
    "MODELS = {\n",
    "    \"Stable Diffusion 2.1\": \"stabilityai/stable-diffusion-2-1\",\n",
    "    \"Stable Diffusion XL\": \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    \"Realistic Vision\": \"SG161222/Realistic_Vision_V4.0\",\n",
    "    \"OpenJourney\": \"prompthero/openjourney\",\n",
    "    \"Dreamlike Diffusion\": \"dreamlike-art/dreamlike-diffusion-1.0\"\n",
    "}\n",
    "\n",
    "# Modify load_model function to handle different models\n",
    "@st.cache_resource\n",
    "def load_model(model_id):\n",
    "    try:\n",
    "        local_path = download_model(model_id)\n",
    "        \n",
    "        # Model-specific parameters\n",
    "        model_params = {\n",
    "            \"stabilityai/stable-diffusion-xl-base-1.0\": {\n",
    "                \"torch_dtype\": torch.float16,\n",
    "                \"use_safetensors\": True,\n",
    "                \"variant\": \"fp16\"\n",
    "            },\n",
    "            \"default\": {\n",
    "                \"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                \"safety_checker\": None,\n",
    "                \"requires_safety_checker\": False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Get model specific params or default\n",
    "        params = model_params.get(model_id, model_params[\"default\"])\n",
    "        \n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            local_path,\n",
    "            local_files_only=True,\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            pipe = pipe.to(\"cuda\")\n",
    "            pipe.enable_attention_slicing()\n",
    "            pipe.enable_xformers_memory_efficient_attention()\n",
    "        return pipe\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Update generate_image function with model-specific parameters\n",
    "def generate_image(pipe, prompt, model_id):\n",
    "    try:\n",
    "        # Model-specific parameters\n",
    "        generation_params = {\n",
    "            \"stabilityai/stable-diffusion-xl-base-1.0\": {\n",
    "                \"num_inference_steps\": 40,\n",
    "                \"guidance_scale\": 9.0,\n",
    "                \"width\": 1024,\n",
    "                \"height\": 1024\n",
    "            },\n",
    "            \"default\": {\n",
    "                \"num_inference_steps\": 50,\n",
    "                \"guidance_scale\": 8.5,\n",
    "                \"width\": 512,\n",
    "                \"height\": 512\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Get model specific params or default\n",
    "        params = generation_params.get(model_id, generation_params[\"default\"])\n",
    "        \n",
    "        enhanced_prompt = f\"high quality, detailed, 4k, {prompt}\"\n",
    "        negative_prompt = \"ugly, blurry, bad quality, dark, low resolution, deformed, distorted\"\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            with torch.autocast(\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "                image = pipe(\n",
    "                    prompt=enhanced_prompt,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                    **params\n",
    "                ).images[0]\n",
    "                \n",
    "                if image is None or image.getextrema() == (0, 0) or image.getextrema() == (255, 255):\n",
    "                    raise Exception(\"Generated image is invalid\")\n",
    "                \n",
    "                return image\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error during image generation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Update main function's model selection\n",
    "def main():\n",
    "    st.title(\"AI Image Generator\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    if check_gpu():\n",
    "        st.success(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        st.info(f\"GPU Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        st.warning(\"No GPU detected. Using CPU (this will be slower)\")\n",
    "    \n",
    "    prompt = st.text_area(\"Enter your prompt (be descriptive):\", \n",
    "                         help=\"Example: A beautiful mountain landscape at sunset, with realistic details, 4k quality\")\n",
    "    \n",
    "    model_name = st.selectbox(\n",
    "        \"Select Model\",\n",
    "        list(MODELS.keys()),\n",
    "        help=\"Different models specialize in different types of images\"\n",
    "    )\n",
    "    \n",
    "    model_id = MODELS[model_name]\n",
    "    \n",
    "    if st.button(\"Generate Image\"):\n",
    "        if not prompt or len(prompt.strip()) < 3:\n",
    "            st.warning(\"Please enter a detailed prompt (at least 3 characters)!\")\n",
    "            return\n",
    "            \n",
    "        with st.spinner(\"Generating image... This may take a minute...\"):\n",
    "            try:\n",
    "                # Clear GPU memory before generation\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                pipe = load_model(model_id)\n",
    "                if pipe is None:\n",
    "                    return\n",
    "                \n",
    "                image = generate_image(pipe, prompt, model_id)\n",
    "                if image is None:\n",
    "                    return\n",
    "                \n",
    "                filepath = save_image(image, prompt)\n",
    "                st.image(image, caption=f\"Generated Image for: {prompt}\", use_container_width=True)\n",
    "                \n",
    "                with open(filepath, \"rb\") as file:\n",
    "                    btn = st.download_button(\n",
    "                        label=\"Download Image\",\n",
    "                        data=file,\n",
    "                        file_name=os.path.basename(filepath),\n",
    "                        mime=\"image/png\"\n",
    "                    )\n",
    "                \n",
    "                st.success(f\"Image saved to: {filepath}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                st.error(f\"Error: {str(e)}\")\n",
    "            finally:\n",
    "                # Clean up memory\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "def download_all_models():\n",
    "    models = [\n",
    "        # \"runwayml/stable-diffusion-v1-5\",\n",
    "        \"stabilityai/stable-diffusion-2-1\"\n",
    "    ]\n",
    "    \n",
    "    for model_id in models:\n",
    "        download_model(model_id)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if st.button(\"Download All Models\"):\n",
    "        download_all_models()\n",
    "        st.success(\"All models downloaded successfully!\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picture edit in not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from datetime import datetime\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Create directories\n",
    "CACHE_DIR = \"model_cache\"\n",
    "OUTPUT_DIR = \"generated_images\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def check_gpu():\n",
    "    \"\"\"Check GPU availability and return status\"\"\"\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error checking GPU: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def save_image(image, prompt):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    safe_prompt = \"\".join(x for x in prompt[:30] if x.isalnum() or x in (' ','-','_')).strip()\n",
    "    filename = f\"{timestamp}_{safe_prompt}.png\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    image.save(filepath)\n",
    "    return filepath\n",
    "\n",
    "def download_model(model_id):\n",
    "    \"\"\"Download model once and cache it\"\"\"\n",
    "    local_path = os.path.join(CACHE_DIR, model_id.split('/')[-1])\n",
    "    \n",
    "    if not os.path.exists(local_path):\n",
    "        st.info(f\"Downloading model {model_id} (this will happen only once)...\")\n",
    "        snapshot_download(\n",
    "            repo_id=model_id,\n",
    "            local_dir=local_path,\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "    return local_path\n",
    "\n",
    "# Add these models to the available options\n",
    "MODELS = {\n",
    "    \"Stable Diffusion 2.1\": \"stabilityai/stable-diffusion-2-1\",\n",
    "    \"Stable Diffusion XL\": \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    \"Realistic Vision\": \"SG161222/Realistic_Vision_V4.0\",\n",
    "    \"OpenJourney\": \"prompthero/openjourney\",\n",
    "    \"Dreamlike Diffusion\": \"dreamlike-art/dreamlike-diffusion-1.0\"\n",
    "}\n",
    "\n",
    "# Modify load_model function to handle different models\n",
    "@st.cache_resource\n",
    "def load_model(model_id):\n",
    "    try:\n",
    "        local_path = download_model(model_id)\n",
    "        \n",
    "        # Model-specific parameters\n",
    "        model_params = {\n",
    "            \"stabilityai/stable-diffusion-xl-base-1.0\": {\n",
    "                \"torch_dtype\": torch.float16,\n",
    "                \"use_safetensors\": True,\n",
    "                \"variant\": \"fp16\"\n",
    "            },\n",
    "            \"default\": {\n",
    "                \"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                \"safety_checker\": None,\n",
    "                \"requires_safety_checker\": False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Get model specific params or default\n",
    "        params = model_params.get(model_id, model_params[\"default\"])\n",
    "        \n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            local_path,\n",
    "            local_files_only=True,\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            pipe = pipe.to(\"cuda\")\n",
    "            pipe.enable_attention_slicing()\n",
    "            pipe.enable_xformers_memory_efficient_attention()\n",
    "        return pipe\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Update generate_image function with model-specific parameters\n",
    "def generate_image(pipe, prompt, model_id):\n",
    "    try:\n",
    "        # Model-specific parameters\n",
    "        generation_params = {\n",
    "            \"stabilityai/stable-diffusion-xl-base-1.0\": {\n",
    "                \"num_inference_steps\": 40,\n",
    "                \"guidance_scale\": 9.0,\n",
    "                \"width\": 1024,\n",
    "                \"height\": 1024\n",
    "            },\n",
    "            \"default\": {\n",
    "                \"num_inference_steps\": 50,\n",
    "                \"guidance_scale\": 8.5,\n",
    "                \"width\": 512,\n",
    "                \"height\": 512\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Get model specific params or default\n",
    "        params = generation_params.get(model_id, generation_params[\"default\"])\n",
    "        \n",
    "        enhanced_prompt = f\"high quality, detailed, 4k, {prompt}\"\n",
    "        negative_prompt = \"ugly, blurry, bad quality, dark, low resolution, deformed, distorted\"\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            with torch.autocast(\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "                image = pipe(\n",
    "                    prompt=enhanced_prompt,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                    **params\n",
    "                ).images[0]\n",
    "                \n",
    "                if image is None or image.getextrema() == (0, 0) or image.getextrema() == (255, 255):\n",
    "                    raise Exception(\"Generated image is invalid\")\n",
    "                \n",
    "                return image\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error during image generation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Update main function's model selection\n",
    "def main():\n",
    "    st.title(\"AI Image Generator\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    if check_gpu():\n",
    "        st.success(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        st.info(f\"GPU Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        st.warning(\"No GPU detected. Using CPU (this will be slower)\")\n",
    "    \n",
    "    prompt = st.text_area(\"Enter your prompt (be descriptive):\", \n",
    "                         help=\"Example: A beautiful mountain landscape at sunset, with realistic details, 4k quality\")\n",
    "    \n",
    "    model_name = st.selectbox(\n",
    "        \"Select Model\",\n",
    "        list(MODELS.keys()),\n",
    "        help=\"Different models specialize in different types of images\"\n",
    "    )\n",
    "    \n",
    "    model_id = MODELS[model_name]\n",
    "    \n",
    "    if st.button(\"Generate Image\"):\n",
    "        if not prompt or len(prompt.strip()) < 3:\n",
    "            st.warning(\"Please enter a detailed prompt (at least 3 characters)!\")\n",
    "            return\n",
    "            \n",
    "        with st.spinner(\"Generating image... This may take a minute...\"):\n",
    "            try:\n",
    "                # Clear GPU memory before generation\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                pipe = load_model(model_id)\n",
    "                if pipe is None:\n",
    "                    return\n",
    "                \n",
    "                image = generate_image(pipe, prompt, model_id)\n",
    "                if image is None:\n",
    "                    return\n",
    "                \n",
    "                filepath = save_image(image, prompt)\n",
    "                st.image(image, caption=f\"Generated Image for: {prompt}\", use_container_width=True)\n",
    "                \n",
    "                with open(filepath, \"rb\") as file:\n",
    "                    btn = st.download_button(\n",
    "                        label=\"Download Image\",\n",
    "                        data=file,\n",
    "                        file_name=os.path.basename(filepath),\n",
    "                        mime=\"image/png\"\n",
    "                    )\n",
    "                \n",
    "                st.success(f\"Image saved to: {filepath}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                st.error(f\"Error: {str(e)}\")\n",
    "            finally:\n",
    "                # Clean up memory\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "def download_all_models():\n",
    "    models = [\n",
    "        # \"runwayml/stable-diffusion-v1-5\",\n",
    "        \"stabilityai/stable-diffusion-2-1\"\n",
    "    ]\n",
    "    \n",
    "    for model_id in models:\n",
    "        download_model(model_id)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if st.button(\"Download All Models\"):\n",
    "        download_all_models()\n",
    "        st.success(\"All models downloaded successfully!\")\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding picture edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from datetime import datetime\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Create directories\n",
    "CACHE_DIR = \"model_cache\"\n",
    "OUTPUT_DIR = \"generated_images\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def check_gpu():\n",
    "    \"\"\"Check GPU availability and return status\"\"\"\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error checking GPU: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def save_image(image, prompt):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    safe_prompt = \"\".join(x for x in prompt[:30] if x.isalnum() or x in (' ','-','_')).strip()\n",
    "    filename = f\"{timestamp}_{safe_prompt}.png\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    image.save(filepath)\n",
    "    return filepath\n",
    "\n",
    "def download_model(model_id):\n",
    "    \"\"\"Download model once and cache it\"\"\"\n",
    "    local_path = os.path.join(CACHE_DIR, model_id.split('/')[-1])\n",
    "    \n",
    "    if not os.path.exists(local_path):\n",
    "        st.info(f\"Downloading model {model_id} (this will happen only once)...\")\n",
    "        snapshot_download(\n",
    "            repo_id=model_id,\n",
    "            local_dir=local_path,\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "    return local_path\n",
    "\n",
    "# Add these models to the available options\n",
    "MODELS = {\n",
    "    \"Stable Diffusion 2.1\": \"stabilityai/stable-diffusion-2-1\",\n",
    "    \"Stable Diffusion XL\": \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    \"Realistic Vision\": \"SG161222/Realistic_Vision_V4.0\",\n",
    "    \"OpenJourney\": \"prompthero/openjourney\",\n",
    "    \"Dreamlike Diffusion\": \"dreamlike-art/dreamlike-diffusion-1.0\"\n",
    "}\n",
    "\n",
    "# Add more image size presets with higher resolutions\n",
    "IMAGE_SIZES = {\n",
    "    \"Square (512x512)\": {\n",
    "        \"width\": 512,\n",
    "        \"height\": 512,\n",
    "        \"description\": \"Standard square format\"\n",
    "    },\n",
    "    \"Square HD (1024x1024)\": {\n",
    "        \"width\": 1024,\n",
    "        \"height\": 1024,\n",
    "        \"description\": \"High resolution square format\"\n",
    "    },\n",
    "    \"Rectangular 16:9 (912x512)\": {\n",
    "        \"width\": 912,\n",
    "        \"height\": 512,\n",
    "        \"description\": \"Widescreen format\"\n",
    "    },\n",
    "    \"Portrait HD (1080x1920)\": {\n",
    "        \"width\": 1080,\n",
    "        \"height\": 1920,\n",
    "        \"description\": \"Mobile/Portrait HD format\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Modify load_model function to handle different models\n",
    "@st.cache_resource\n",
    "def load_model(model_id):\n",
    "    try:\n",
    "        local_path = download_model(model_id)\n",
    "        \n",
    "        # Model-specific parameters\n",
    "        model_params = {\n",
    "            \"stabilityai/stable-diffusion-xl-base-1.0\": {\n",
    "                \"torch_dtype\": torch.float16,\n",
    "                \"use_safetensors\": True,\n",
    "                \"variant\": \"fp16\"\n",
    "            },\n",
    "            \"default\": {\n",
    "                \"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                \"safety_checker\": None,\n",
    "                \"requires_safety_checker\": False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Get model specific params or default\n",
    "        params = model_params.get(model_id, model_params[\"default\"])\n",
    "        \n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            local_path,\n",
    "            local_files_only=True,\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            pipe = pipe.to(\"cuda\")\n",
    "            pipe.enable_attention_slicing()\n",
    "            pipe.enable_xformers_memory_efficient_attention()\n",
    "        return pipe\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Update generation parameters for high-res images\n",
    "def generate_image(pipe, prompt, model_id, size_preset):\n",
    "    try:\n",
    "        # Model-specific parameters with high-res adjustments\n",
    "        generation_params = {\n",
    "            \"stabilityai/stable-diffusion-xl-base-1.0\": {\n",
    "                \"num_inference_steps\": 50,  # Increased for better quality\n",
    "                \"guidance_scale\": 9.0,\n",
    "                \"width\": IMAGE_SIZES[size_preset][\"width\"],\n",
    "                \"height\": IMAGE_SIZES[size_preset][\"height\"]\n",
    "            },\n",
    "            \"default\": {\n",
    "                \"num_inference_steps\": 50,\n",
    "                \"guidance_scale\": 8.5,\n",
    "                \"width\": IMAGE_SIZES[size_preset][\"width\"],\n",
    "                \"height\": IMAGE_SIZES[size_preset][\"height\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add warning for high-res images\n",
    "        if IMAGE_SIZES[size_preset][\"width\"] * IMAGE_SIZES[size_preset][\"height\"] > 512 * 512:\n",
    "            st.warning(\"Generating high-resolution image. This may take longer and require more GPU memory.\")\n",
    "        \n",
    "        enhanced_prompt = f\"high quality, detailed, 4k, {prompt}\"\n",
    "        negative_prompt = \"ugly, blurry, bad quality, dark, low resolution, deformed, distorted\"\n",
    "        \n",
    "        # Get model specific params or default\n",
    "        params = generation_params.get(model_id, generation_params[\"default\"])\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            with torch.autocast(\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "                image = pipe(\n",
    "                    prompt=enhanced_prompt,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                    **params  # Now params is defined\n",
    "                ).images[0]\n",
    "                \n",
    "                if image is None or image.getextrema() == (0, 0) or image.getextrema() == (255, 255):\n",
    "                    raise Exception(\"Generated image is invalid\")\n",
    "                \n",
    "                return image\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error during image generation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Update main function's model selection\n",
    "def main():\n",
    "    st.title(\"AI Image Generator\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    if check_gpu():\n",
    "        st.success(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        st.info(f\"GPU Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        st.warning(\"No GPU detected. Using CPU (this will be slower)\")\n",
    "    \n",
    "    # Create columns for input options\n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        model_name = st.selectbox(\n",
    "            \"Select Model\",\n",
    "            list(MODELS.keys()),\n",
    "            help=\"Different models specialize in different types of images\"\n",
    "        )\n",
    "    \n",
    "    with col2:\n",
    "        size_preset = st.selectbox(\n",
    "            \"Image Size\",\n",
    "            list(IMAGE_SIZES.keys()),\n",
    "            help=\"Choose the aspect ratio and size of the generated image\"\n",
    "        )\n",
    "    \n",
    "    prompt = st.text_area(\n",
    "        \"Enter your prompt (be descriptive):\", \n",
    "        help=\"Example: A beautiful mountain landscape at sunset, with realistic details, 4k quality\"\n",
    "    )\n",
    "    \n",
    "    model_id = MODELS[model_name]\n",
    "    \n",
    "    # Display selected size info\n",
    "    st.info(f\"Selected size: {IMAGE_SIZES[size_preset]['width']}x{IMAGE_SIZES[size_preset]['height']} - {IMAGE_SIZES[size_preset]['description']}\")\n",
    "    \n",
    "    if st.button(\"Generate Image\"):\n",
    "        if not prompt or len(prompt.strip()) < 3:\n",
    "            st.warning(\"Please enter a detailed prompt (at least 3 characters)!\")\n",
    "            return\n",
    "            \n",
    "        with st.spinner(\"Generating image... This may take a minute...\"):\n",
    "            try:\n",
    "                # Clear GPU memory before generation\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                pipe = load_model(model_id)\n",
    "                if pipe is None:\n",
    "                    return\n",
    "                \n",
    "                image = generate_image(pipe, prompt, model_id, size_preset)\n",
    "                if image is None:\n",
    "                    return\n",
    "                \n",
    "                filepath = save_image(image, prompt)\n",
    "                st.image(image, caption=f\"Generated Image for: {prompt}\", use_container_width=True)\n",
    "                \n",
    "                with open(filepath, \"rb\") as file:\n",
    "                    btn = st.download_button(\n",
    "                        label=\"Download Image\",\n",
    "                        data=file,\n",
    "                        file_name=os.path.basename(filepath),\n",
    "                        mime=\"image/png\"\n",
    "                    )\n",
    "                \n",
    "                st.success(f\"Image saved to: {filepath}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                st.error(f\"Error: {str(e)}\")\n",
    "            finally:\n",
    "                # Clean up memory\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "def download_all_models():\n",
    "    models = [\n",
    "        # \"runwayml/stable-diffusion-v1-5\",\n",
    "        \"stabilityai/stable-diffusion-2-1\"\n",
    "    ]\n",
    "    \n",
    "    for model_id in models:\n",
    "        download_model(model_id)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if st.button(\"Download All Models\"):\n",
    "        download_all_models()\n",
    "        st.success(\"All models downloaded successfully!\")\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
